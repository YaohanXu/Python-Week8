{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <br>Analyzing and Visualizing Large Datasets\n",
    "\n",
    "- Nov 14 2024 (continued from last week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We continue working with large datasets \n",
    "\n",
    "**By example:**\n",
    "- Census data \n",
    "- NYC taxi cab trips\n",
    "- Parking violations in NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fresh Environment (do in bash)\n",
    "\n",
    "- You may have (every now and then) dependency conflicts due to incompatible package versions (same in R!!)\n",
    "- Use a Fresh Virtual Environment: Creating a new environment is the safest way to avoid conflicts. If you're using conda, create an environment and install packages sequentially.\n",
    "  - conda create -n myenv python=3.10\n",
    "  - conda activate myenv\n",
    "\n",
    "- Install Packages with Compatible Versions\n",
    "  - pip install numpy==1.24.2 pandas==1.5.3 pip install bokeh==3.2.2 panel==1.0.4 holoviews==1.16.0 geoviews==1.13.0 pip install datashader==0.16.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "- Import datashader and related modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# some first class upgrades\n",
    "#!pip install numpy --upgrade\n",
    "#!pip install datashader --upgrade\n",
    "\n",
    "\n",
    "# Initial imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "import zipfile\n",
    "import requests\n",
    "import dask.dataframe as dd\n",
    "import fastparquet\n",
    "\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Color-related imports\n",
    "from datashader.colors import Greys9, viridis, inferno\n",
    "from colorcet import fire\n",
    "\n",
    "# Ignore numpy warnings\n",
    "np.seterr(\"ignore\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Download and Extract: \n",
    "\n",
    "- First, download and unzip the file using Python's zipfile module or another tool.\n",
    "- So here we work with the dataset locally (better if your internet is slow)\n",
    "- Attention, that download can take time (3-4 minutes) - the census file is about 2GB\n",
    "- Once extracted, Dask can read the actual parquet file.\n",
    "- Then we read the parquet file and convert to a dask dataframe\n",
    "- There are sometimes warnings, but don't worry about them for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Download and extract\n",
    "#url = \"https://s3.amazonaws.com/datashader-data/census2010.parq.zip\"\n",
    "#response = requests.get(url)\n",
    "#with open(\"census2010.parq.zip\", \"wb\") as f:\n",
    "#    f.write(response.content)\n",
    "\n",
    "# print(\"ok\")\n",
    "\n",
    "# Extract the file\n",
    "with zipfile.ZipFile(\"census2010.parq.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"census_data\")\n",
    "\n",
    "# Read with pyarrow\n",
    "table = pq.read_table(\"census_data/census2010.parq\")\n",
    "\n",
    "# Convert to a Pandas DataFrame\n",
    "df = table.to_pandas()\n",
    "\n",
    "# Then, convert to a Dask DataFrame\n",
    "census_ddf = dd.from_pandas(df, npartitions=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into the structure of the dask dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "census_ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "census_ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plotting time!!!\n",
    "Setup canvas parameters for USA image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from datashader.utils import lnglat_to_meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Sensible lat/lng coordinates for U.S. cities\n",
    "# NOTE: these are in lat/lng so EPSG=4326\n",
    "USA = [(-124.72,  -66.95), (23.55, 50.06)]\n",
    "\n",
    "# Get USA xlim and ylim in meters (EPSG=3857)\n",
    "USA_xlim_meters, USA_ylim_meters = [list(r) for r in lnglat_to_meters(USA[0], USA[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define some a default plot width & height\n",
    "plot_width  = 900\n",
    "plot_height = int(plot_width*7.0/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting our ddf to a Panda DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a Pandas DataFrame\n",
    "df = table.to_pandas()\n",
    "\n",
    "# Convert the 'race' column to categorical if itâ€™s not already\n",
    "df['race'] = df['race'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use a custom color scheme to map racial demographics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "color_key = {\"w\": \"aqua\", \"b\": \"lime\", \"a\": \"red\", \"h\": \"fuchsia\", \"o\": \"yellow\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_census_image(longitude_range, latitude_range, w=plot_width, h=plot_height):\n",
    "    \"\"\"\n",
    "    A function for plotting the Census data, coloring pixel by race values.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate x and y range from lng/lat ranges\n",
    "    x_range, y_range = lnglat_to_meters(longitude_range, latitude_range)\n",
    "\n",
    "    # Step 2: Setup the canvas\n",
    "    canvas = ds.Canvas(plot_width=w, plot_height=h, x_range=x_range, y_range=y_range)\n",
    "\n",
    "    # Step 3: Aggregate, but this time count the \"race\" category\n",
    "    # NEW: specify the aggregation method to count the \"race\" values in each pixel\n",
    "    agg = canvas.points(df, \"easting\", \"northing\", agg=ds.count_cat(\"race\"))\n",
    "    \n",
    "    # Step 4: Shade, using our custom color map\n",
    "    img = tf.shade(agg, color_key=color_key, how=\"eq_hist\")\n",
    "\n",
    "    # Return image with black background\n",
    "    return tf.set_background(img, \"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = create_census_image((-132.84029, -64.533035), (22.182915, 51.35972), w=1000, h=600)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can we learn more than just population density and race?\n",
    "\n",
    "We can use *xarray* to slice the array of aggregated pixel values to examine specific aspects of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question #1: Where do African Americans live?\n",
    "\n",
    "Use the `sel()` function of the *xarray* array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Setup canvas\n",
    "cvs = ds.Canvas(plot_width=plot_width, plot_height=plot_height)\n",
    "\n",
    "census_ddf['race'] = census_ddf['race'].astype('category')\n",
    "# Step 2: Aggregate and count race category\n",
    "aggc = cvs.points(census_ddf, \"easting\", \"northing\", agg=ds.count_cat(\"race\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aggc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NEW: Select only African Americans (where \"race\" column is equal to \"b\")\n",
    "agg_b = aggc.sel(race=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agg_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Shade and set background\n",
    "img = tf.shade(agg_b, cmap=fire, how=\"eq_hist\")\n",
    "img = tf.set_background(img, \"black\")\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question #2: How to identify diverse areas?\n",
    "\n",
    "**Goal:** Select pixels where each race has a non-zero count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aggc.sel(race=[\"w\", \"b\", \"a\", \"h\"]) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "(aggc.sel(race=['w', 'b', 'a', 'h']) > 0).all(dim='race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Do a \"logical and\" operation across the \"race\" dimension\n",
    "# Pixels will be \"True\" if the pixel has a positive count for each race\n",
    "diverse_selection = (aggc.sel(race=['w', 'b', 'a', 'h']) > 200).all(dim='race')\n",
    "\n",
    "diverse_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Select the pixel values where our diverse selection criteria is True\n",
    "agg2 = aggc.where(diverse_selection).fillna(0)\n",
    "\n",
    "# and shade using our color key\n",
    "img = tf.shade(agg2, color_key=color_key)\n",
    "img = tf.set_background(img,\"black\")\n",
    "\n",
    "img "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question #3: Where is African American population greater than the White population?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Select where the \"b\" race dimension is greater than the \"w\" race dimension\n",
    "selection = aggc.sel(race='h') > 4*(aggc.sel(race='w'))\n",
    "\n",
    "selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Select based on the selection criteria\n",
    "agg3 = aggc.where(selection).fillna(0)\n",
    "\n",
    "img = tf.shade(agg3, color_key=color_key)\n",
    "img = tf.set_background(img, \"black\")\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now let's make it interactive!\n",
    "\n",
    "\n",
    "**Let's use hvplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize hvplot and dask\n",
    "import hvplot.pandas\n",
    "import hvplot.dask # NEW: dask works with hvplot too!\n",
    "\n",
    "import holoviews as hv\n",
    "import geoviews as gv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Side note: persisting dask arrays in memory\n",
    "\n",
    "To speed up interactive calculations, you can \"persist\" a dask array in memory (load the data fully into memory). You should have at least 16 GB of memory to avoid memory errors, though!\n",
    "\n",
    "If not persisted, the data will be loaded on demand to avoid memory issues, which will slow the interactive nature of the plots down slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_ddf = census_ddf.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we scaled our coordinates, so we only focus on that area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_coordinatesX = (-13983766.89173708, -6883766.89173708)\n",
    "scaled_coordinatesY = (2552839.908609666, 6256673.275328225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import hvplot.dask\n",
    "import geoviews as gv\n",
    "from holoviews import opts\n",
    "\n",
    "# Ensure correct coordinate boundaries in meters for EPSG:3857\n",
    "# Using previously defined USA_xlim_meters and USA_ylim_meters\n",
    "\n",
    "# Plot the points using hvplot with datashader\n",
    "points = census_ddf.hvplot.points(\n",
    "    x=\"easting\",\n",
    "    y=\"northing\",\n",
    "    datashade=True,\n",
    "    aggregator=ds.count(),\n",
    "    cmap=fire,\n",
    "    geo=True,\n",
    "    crs=\"EPSG:3857\",  # Specify that the input data is in EPSG:3857\n",
    "    frame_width=plot_width,\n",
    "    frame_height=plot_height,\n",
    "    xlim=scaled_coordinatesX ,  # Set x bounds in meters\n",
    "    ylim=scaled_coordinatesY,  # Set y bounds in meters\n",
    ")\n",
    "\n",
    "# Define the background tile source without specifying `crs`\n",
    "bg = gv.tile_sources.CartoDark.options(width=plot_width, height=plot_height)\n",
    "\n",
    "# Combine the tile source with the points plot\n",
    "(bg * points).opts(\n",
    "    opts.Tiles(xlim=scaled_coordinatesX , ylim=scaled_coordinatesY)  # Set x and y limits on the tile source as well\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note:** interactive features (panning, zooming, etc) can be *slow*, but the map will eventually re-load!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can visualize color-coded race interactively as well\n",
    "\n",
    "- Similar syntax to previous examples...\n",
    "- Note how the `cmap` changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Points with categorical colormap\n",
    "race_map = census_ddf.hvplot.points(\n",
    "    x=\"easting\",\n",
    "    y=\"northing\",\n",
    "    datashade=True,\n",
    "    c=\"race\",  # NEW: color pixels by \"race\" column\n",
    "    aggregator=ds.count_cat(\"race\"),  # NEW: specify the aggregator\n",
    "    cmap=color_key,  # NEW: use our custom color map dictionary\n",
    "    crs=3857,\n",
    "    geo=True,\n",
    "    frame_width=plot_width,\n",
    "    frame_height=plot_height,\n",
    "    xlim=scaled_coordinatesX,\n",
    "    ylim=scaled_coordinatesY,\n",
    ")\n",
    "\n",
    "bg = gv.tile_sources.CartoDark\n",
    "\n",
    "bg * race_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use case: exploring gerrymandering\n",
    "\n",
    "- When we use examples of population, together with race or other demographics....\n",
    "- Are some districts are drawn according to race distribution?\n",
    "- We can easily overlay Congressional districts on our map..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load congressional districts and convert to EPSG=3857\n",
    "districts = gpd.read_file('./data/cb_2015_us_cd114_5m').to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(let's check if the districts are correctly mapped), and if necessary, just plot the ones for the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas  # This enables hvplot for both pandas and geopandas\n",
    "\n",
    "districts_map = districts.hvplot.polygons(\n",
    "    geo=True,\n",
    "    crs=\"EPSG:3857\",  # Ensure the CRS matches the plot\n",
    "    line_color=\"white\",\n",
    "    fill_alpha=0.5,\n",
    "    title=\"District Map\"\n",
    ")\n",
    "\n",
    "districts_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the district map\n",
    "districts_map = districts.hvplot.polygons(\n",
    "    geo=True,\n",
    "    crs=3857,\n",
    "    line_color=\"white\",\n",
    "    fill_alpha=0,\n",
    "    frame_width=plot_width,\n",
    "    frame_height=plot_height,\n",
    "    xlim=scaled_coordinatesX,\n",
    "    ylim=scaled_coordinatesY\n",
    "    \n",
    ")\n",
    "\n",
    "bg * districts_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_map = census_ddf.hvplot.points(\n",
    "    x=\"easting\",\n",
    "    y=\"northing\",\n",
    "    datashade=True,\n",
    "    c=\"race\",\n",
    "    aggregator=ds.count_cat(\"race\"),\n",
    "    cmap=color_key,\n",
    "    geo=True,\n",
    "    crs=\"EPSG:3857\",  # Explicitly specify the CRS\n",
    "    frame_width=plot_width,\n",
    "    frame_height=plot_height,\n",
    "    xlim=USA_xlim_meters,\n",
    "    ylim=USA_ylim_meters,\n",
    ")\n",
    "\n",
    "# Ensure the background and district maps are in EPSG:3857 as well\n",
    "bg = gv.tile_sources.CartoDark.opts(xlim=scaled_coordinatesX, ylim=scaled_coordinatesY)\n",
    "districts_map = districts_map.opts(xlim=scaled_coordinatesX, ylim=scaled_coordinatesY)\n",
    "\n",
    "# Combine the maps\n",
    "img = bg * race_map * districts_map\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example 3: NYC taxi data\n",
    "\n",
    "12 million taxi trips from 2015..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Download the file\n",
    "url = \"https://s3.amazonaws.com/datashader-data/nyc_taxi_wide.parq\"\n",
    "response = requests.get(url)\n",
    "with open(\"nyc_taxi_wide.parq\", \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "taxi_ddf = dd.read_parquet(\"nyc_taxi_wide.parq\", engine=\"pyarrow\", \n",
    "                               infer_divisions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "df = dd.read_parquet(\"nyc_taxi_wide.parq\", engine=\"fastparquet\")\n",
    "    \n",
    "# Save the dataframe without using RLE encoding\n",
    "df.to_parquet(\"nyc_taxi_wide_reencoded.parq\", engine=\"fastparquet\", compression=\"SNAPPY\", overwrite=True)\n",
    "\n",
    "\n",
    "print(\"File successfully re-saved without RLE encoding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check our data \n",
    "- can you recognize the different columns, and what they mean?\n",
    "- what would you be interested in mapping?\n",
    "- could you map derived variables\n",
    "  - trip length (by region, by time)\n",
    "  - trip duration\n",
    "  - origin - destination at different time of day\n",
    "  - ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = df.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{len(df)} Rows\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim your data\n",
    "- What would you do if the data was extremely large?\n",
    "- 64bit, 32bit??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Trim to the columns\n",
    "df = df[\n",
    "    [\n",
    "        \"passenger_count\",\n",
    "        \"pickup_x\",\n",
    "        \"pickup_y\",\n",
    "        \"dropoff_x\",\n",
    "        \"dropoff_y\",\n",
    "        \"dropoff_hour\",\n",
    "        \"pickup_hour\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import hvplot.dask  # for Dask + hvPlot integration\n",
    "import geoviews as gv\n",
    "import hvplot.pandas  # for hvPlot functionality\n",
    "\n",
    "\n",
    "# Create the map visualization\n",
    "pickups_map = df.hvplot.points(\n",
    "    x=\"pickup_x\",\n",
    "    y=\"pickup_y\",\n",
    "    color=\"blue\",\n",
    "    frame_width=800,\n",
    "    frame_height=600,\n",
    "    geo=True,\n",
    "    crs=\"EPSG:3857\",\n",
    "    tiles=\"CartoDark\"  # Adding tiles directly here for convenience\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "pickups_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploring the taxi pick ups..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by the hour column to add a slider widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickups_map = df.hvplot.points(\n",
    "    x=\"pickup_x\",\n",
    "    y=\"pickup_y\",\n",
    "    groupby=\"pickup_hour\",\n",
    "    cmap=fire,\n",
    "    datashade=True,\n",
    "    frame_width=800,\n",
    "    frame_height=600,\n",
    "    geo=True, \n",
    "    crs=3857\n",
    ")\n",
    "\n",
    "gv.tile_sources.CartoDark * pickups_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing pickups and dropoffs\n",
    "\n",
    "- Pixels with more pickups: *shaded red*\n",
    "- Pixels with more dropoffs: *shaded blue*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Bounds in meters for NYC (EPSG=3857)\n",
    "NYC = [(-8242000,-8210000), (4965000,4990000)]\n",
    "\n",
    "# Set a plot width and height\n",
    "plot_width  = int(750)\n",
    "plot_height = int(plot_width//1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = plot_width\n",
    "h = plot_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_range = NYC[0]\n",
    "y_range = NYC[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Create the canvas\n",
    "cvs = ds.Canvas(plot_width=w, plot_height=h, x_range=x_range, y_range=y_range)\n",
    "\n",
    "# Step 2: Aggregate the pick ups\n",
    "picks = cvs.points(df, \"pickup_x\", \"pickup_y\", ds.count(\"passenger_count\"))\n",
    "\n",
    "# Step 2: Aggregate the drop offs\n",
    "drops = cvs.points(df, \"dropoff_x\", \"dropoff_y\", ds.count(\"passenger_count\"))\n",
    "\n",
    "# Rename to same names\n",
    "drops = drops.rename({\"dropoff_x\": \"x\", \"dropoff_y\": \"y\"})\n",
    "picks = picks.rename({\"pickup_x\": \"x\", \"pickup_y\": \"y\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def create_merged_taxi_image(\n",
    "    x_range, y_range, w=plot_width, h=plot_height, how=\"eq_hist\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a merged taxi image, showing areas with: \n",
    "    \n",
    "    - More pickups than dropoffs in red\n",
    "    - More dropoffs than pickups in blue\n",
    "    \"\"\"\n",
    "    # Step 1: Create the canvas\n",
    "    cvs = ds.Canvas(plot_width=w, plot_height=h, x_range=x_range, y_range=y_range)\n",
    "\n",
    "    # Step 2: Aggregate the pick ups\n",
    "    picks = cvs.points(df, \"pickup_x\", \"pickup_y\", ds.count(\"passenger_count\"))\n",
    "\n",
    "    # Step 2: Aggregate the drop offs\n",
    "    drops = cvs.points(df, \"dropoff_x\", \"dropoff_y\", ds.count(\"passenger_count\"))\n",
    "\n",
    "    # Rename to same names\n",
    "    drops = drops.rename({\"dropoff_x\": \"x\", \"dropoff_y\": \"y\"})\n",
    "    picks = picks.rename({\"pickup_x\": \"x\", \"pickup_y\": \"y\"})\n",
    "\n",
    "    # Step 3: Shade\n",
    "    # NEW: shade pixels there are more drop offs than pick ups\n",
    "    # These are colored blue\n",
    "    more_drops = tf.shade(\n",
    "        drops.where(drops > picks), cmap=[\"darkblue\", \"cornflowerblue\"], how=how\n",
    "    )\n",
    "\n",
    "    # Step 3: Shade\n",
    "    # NEW: shade pixels where there are more pick ups than drop offs\n",
    "    # These are colored red\n",
    "    more_picks = tf.shade(\n",
    "        picks.where(picks > drops), cmap=[\"darkred\", \"orangered\"], how=how\n",
    "    )\n",
    "\n",
    "    # Step 4: Combine\n",
    "    # NEW: add the images together!\n",
    "    img = tf.stack(more_picks, more_drops)\n",
    "\n",
    "    return tf.set_background(img, \"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGA = [(-8228000, -8220000), (4977000, 4980000)]\n",
    "create_merged_taxi_image(LGA[0], LGA[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "create_merged_taxi_image(NYC[0], NYC[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Takeaway:** pickups occur more often on major roads, and dropoffs on smaller roads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's generate a time-lapse GIF of drop-offs over time\n",
    "\n",
    "Powerful tool for visualizing trends over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define some functions...\n",
    "\n",
    "**Important:** We can convert our datashaded images to the format of the Python Imaging Library (PIL) to visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def create_taxi_image(df, x_range, y_range, w=plot_width, h=plot_height, cmap=fire):\n",
    "    \"\"\"Create an image of taxi dropoffs, returning a Python Imaging Library (PIL) image.\"\"\"\n",
    "    \n",
    "    # Step 1: Create the canvas\n",
    "    cvs = ds.Canvas(plot_width=w, plot_height=h, x_range=x_range, y_range=y_range)\n",
    "    \n",
    "    # Step 2: Aggregate the dropoff positions, coutning number of passengers\n",
    "    agg = cvs.points(df, 'dropoff_x', 'dropoff_y',  ds.count('passenger_count'))\n",
    "    \n",
    "    # Step 3: Shade\n",
    "    img = tf.shade(agg, cmap=cmap, how='eq_hist')\n",
    "    \n",
    "    # Set the background\n",
    "    img = tf.set_background(img, \"black\")\n",
    "    \n",
    "    # NEW: return an PIL image\n",
    "    return img.to_pil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_12hour(hr24):\n",
    "    \"\"\"Convert from 24 hr to 12 hr.\"\"\"\n",
    "    from datetime import datetime\n",
    "    d = datetime.strptime(str(hr24), \"%H\")\n",
    "    return d.strftime(\"%I %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_dropoffs_by_hour(fig, data_all_hours, hour, x_range, y_range):\n",
    "    \"\"\"Plot the dropoffs for particular hour.\"\"\"\n",
    "    \n",
    "    # Trim to the specific hour\n",
    "    df_this_hour = data_all_hours.loc[data_all_hours[\"dropoff_hour\"] == hour]\n",
    "\n",
    "    # Create the datashaded image for this hour\n",
    "    img = create_taxi_image(df_this_hour, x_range, y_range)\n",
    "\n",
    "    # Plot the image on a matplotlib axes\n",
    "    # Use imshow()\n",
    "    plt.clf()\n",
    "    ax = fig.gca()\n",
    "    ax.imshow(img, extent=[x_range[0], x_range[1], y_range[0], y_range[1]])\n",
    "    \n",
    "    # Format the axis and figure\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_axis_off()\n",
    "    fig.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "\n",
    "    # Optional: Add a text label for the hour\n",
    "    ax.text(\n",
    "        0.05,\n",
    "        0.9,\n",
    "        convert_to_12hour(hour),\n",
    "        color=\"white\",\n",
    "        fontsize=40,\n",
    "        ha=\"left\",\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "\n",
    "    # Draw the figure and return the image\n",
    "    # This converts our matplotlib Figure into a format readable by imageio\n",
    "    fig.canvas.draw()\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=\"uint8\")\n",
    "    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Strategy:\n",
    "\n",
    "1. Create a datashaded image for each hour of taxi dropoffs, return as a PIL image object\n",
    "1. Use matplotlib's `imshow()` to plot each datashaded image to a matplotlib Figure\n",
    "1. Return each matplotlib Figure in a format readable by the `imageio` library \n",
    "1. Combine all of our images for each hours into a GIF using the `imageio` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig, ax = plt.subplots(figsize=(10, 10), facecolor=\"black\")\n",
    "\n",
    "# Create an image for each hour\n",
    "imgs = []\n",
    "for hour in range(24):\n",
    "\n",
    "    # Plot the datashaded image for this specific hour\n",
    "    print(hour)\n",
    "    img = plot_dropoffs_by_hour(fig, taxi_ddf, hour, x_range=NYC[0], y_range=NYC[1])\n",
    "    imgs.append(img)\n",
    "\n",
    "\n",
    "# Combing the images for each hour into a single GIF\n",
    "imageio.mimsave(\"dropoffsB.gif\", imgs, duration=1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interesting aside: Beyond hvplot\n",
    "\n",
    "**Analyzing hourly and weekly trends for taxis using holoviews**\n",
    "\n",
    "- We'll load taxi data from 2016 that includes the number of pickups per hour.\n",
    "- Visualize weekly and hourly trends using a *radial heatmap*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/nyc_taxi_2016_by_hour.csv.gz', parse_dates=['Pickup_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Add date-related columns with `strftime`\n",
    "\n",
    "We can use the `strftime()` function of a datetime Series to extract specific aspects of a date object. In this case, we are interested in:\n",
    "\n",
    "- Day of Week (Monday, Tuesday, etc) and hour of day\n",
    "- Week of Year\n",
    "\n",
    "To find the specific string notation for these, use: http://strftime.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create relevant time columns\n",
    "df[\"Day & Hour\"] = df[\"Pickup_date\"].dt.strftime(\"%A %H:00\")\n",
    "df[\"Week of Year\"] = df[\"Pickup_date\"].dt.strftime(\"Week %W\")\n",
    "df[\"Date\"] = df[\"Pickup_date\"].dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's plot a radial heatmap\n",
    "\n",
    "The binning dimensions of the heat map will be:\n",
    "\n",
    "- Day of Week and Hour of Day\n",
    "- Week of Year\n",
    "\n",
    "A radial heatmap can be read similar to tree rings:\n",
    "\n",
    "- The center of the heatmap will represent the first week of the year, while the outer edge is the last week of the year\n",
    "- Rotating clockwise along a specific ring tells you the day/hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')  # Ensure that you have loaded the holoviews extension for plotting\n",
    "\n",
    "# Define the columns you want to use\n",
    "cols = [\"Day & Hour\", \"Week of Year\", \"Pickup_Count\", \"Date\"]\n",
    "\n",
    "# Create the heatmap\n",
    "heatmap = hv.HeatMap(\n",
    "    df[cols], kdims=[\"Day & Hour\", \"Week of Year\"], vdims=[\"Pickup_Count\", \"Date\"]\n",
    ")\n",
    "\n",
    "# Customize heatmap options (example adjustments)\n",
    "heatmap.opts(\n",
    "    width=600, height=400,  # Set plot dimensions\n",
    "    colorbar=True,          # Add a colorbar for clarity\n",
    "    cmap=\"Viridis\",         # Choose a color map, e.g., 'Viridis'\n",
    "    tools=['hover'],        # Enable hover tool for interactivity\n",
    "    xlabel='Day & Hour',\n",
    "    ylabel='Week of Year',\n",
    "    title='Heatmap of Pickup Counts by Day & Hour and Week of Year'\n",
    ")\n",
    "\n",
    "# Display the heatmap\n",
    "heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "heatmap.opts(\n",
    "    radial=True,\n",
    "    height=600,\n",
    "    width=600,\n",
    "    yticks=None,\n",
    "    xmarks=7,\n",
    "    ymarks=3,\n",
    "    start_angle=np.pi * 19 / 14,\n",
    "    xticks=(\n",
    "        \"Friday\",\n",
    "        \"Saturday\",\n",
    "        \"Sunday\",\n",
    "        \"Monday\",\n",
    "        \"Tuesday\",\n",
    "        \"Wednesday\",\n",
    "        \"Thursday\",\n",
    "    ),\n",
    "    tools=[\"hover\"],\n",
    "    cmap=\"fire\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Trends\n",
    "\n",
    "- Taxi pickup counts are high between 7-9am and 5-10pm during weekdays which business hours as expected. In contrast, during weekends, there is not much going on until 11am.\n",
    "- **Friday and Saterday nights clearly stand out with the highest pickup densities** as expected.\n",
    "- Public holidays can be easily identified. For example, taxi pickup counts are comparetively low around Christmas and Thanksgiving.\n",
    "- Weather phenomena also influence taxi service. There is a very dark stripe at the beginning of the year starting at Saturday 23rd and lasting until Sunday 24th. Interestingly, there was one of the biggest blizzards in the history of NYC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Useful reference: the Holoviews example gallery\n",
    "\n",
    "This radial heatmap example, and many more examples beyond hvplot available:\n",
    "\n",
    "[https://holoviews.org/gallery/index.html](https://holoviews.org/gallery/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Datashading Philly parking violations data\n",
    "\n",
    "#### Download the data\n",
    "\n",
    "- A (large) CSV of parking violation data is available for download at: [https://musa550.s3.amazonaws.com/parking_violations.csv](https://musa550.s3.amazonaws.com/parking_violations.csv)\n",
    "- Navigate to your browser, plug in the above URL, and download the data\n",
    "- The data is from Open Data Philly: [https://www.opendataphilly.org/dataset/parking-violations](https://www.opendataphilly.org/dataset/parking-violations)\n",
    "- Input data is in EPSG=4326\n",
    "- **Remember**: You will need to convert latitude/longitude to Web Mercator (epsg=3857) to work with datashader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1: Use `dask` to load the data\n",
    "\n",
    "- The `dask.dataframe` module includes a [`read_csv()`](https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.read_csv) function just like pandas \n",
    "- You'll want to specify the `assume_missing=True` keyword for that function: that will let dask know that some columns are allowed to have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I downloaded the data and moved it to the \"data/\" folder \n",
    "df = dd.read_csv(\"data/parking_violations.csv\", assume_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 2: Remove any rows with missing geometries\n",
    "\n",
    "Remove rows that have NaN for either the `lat` or `lon` columns (*hint*: use the dropna() function!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 3: Convert lat/lng to Web Mercator coordinates (x, y)\n",
    "\n",
    "Add two new columns, `x` and `y`, that represent the coordinates in the EPSG=3857 CRS.\n",
    "\n",
    "**Hint:** Use datashader's `lnglat_to_meters()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datashader.utils import lnglat_to_meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do the conversion\n",
    "x, y = lnglat_to_meters(df['lon'], df['lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add as columns\n",
    "df['x'] = x\n",
    "df['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 4: Get the x/y range for Philadelphia for our canvas\n",
    "\n",
    "- Convert the lat/lng bounding box into Web Mercator EPSG=3857 \n",
    "- Use the `lnglat_to_meters()` function to do the conversion\n",
    "- You should have two variables `x_range` and `y_range` that give you the corresponding `x` and `y` bounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Use lat/lng bounds for Philly\n",
    "# This will exclude any points that fall outside this region\n",
    "PhillyBounds = [( -75.28,  -74.96), (39.86, 40.14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PhillyBoundsLng = PhillyBounds[0]\n",
    "PhillyBoundsLat = PhillyBounds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to an EPSG=3857\n",
    "x_range, y_range = lnglat_to_meters(PhillyBoundsLng, PhillyBoundsLat)\n",
    "\n",
    "x_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional: convert to lists as opposed to arrays\n",
    "x_range = list(x_range)\n",
    "y_range = list(y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 5: Datashade the dataset\n",
    " \n",
    "Create a matplotlib figure with the datashaded image of the parking violation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 1: Create the canvas\n",
    "cvs = ds.Canvas(plot_width=600, plot_height=600, x_range=x_range, y_range=y_range)\n",
    "\n",
    "# STEP 2: Aggregate the points\n",
    "agg = cvs.points(df, \"x\", \"y\", agg=ds.count())\n",
    "\n",
    "# STEP 3: Shade the aggregated pixels\n",
    "img = tf.shade(agg, cmap=fire, how=\"eq_hist\")\n",
    "\n",
    "# Optional: Set the background of the image\n",
    "img = tf.set_background(img, \"black\")\n",
    "\n",
    "# Show!\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's add the city limits.\n",
    "\n",
    "You'll need to convert your datashaded image to PIL format and use the `imshow()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "city_limits = gpd.read_file(\"./data/City_Limits.geojson\")\n",
    "\n",
    "# Same CRS!\n",
    "city_limits = city_limits.to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show with matplotlib\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# NEW:\n",
    "ax.imshow(img.to_pil(), extent=[x_range[0], x_range[1], y_range[0], y_range[1]])\n",
    "\n",
    "# Format\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Add the city limits on top!\n",
    "city_limits.plot(ax=ax, facecolor=\"none\", edgecolor=\"white\", linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 6: Make an interactive map\n",
    "\n",
    "Use hvplot to make an interactive version of your datashaded image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "violations = df.hvplot.points(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    datashade=True,\n",
    "    geo=True,\n",
    "    crs=3857,\n",
    "    frame_width=600,\n",
    "    frame_height=600,\n",
    "    cmap=fire,\n",
    "    xlim=x_range,\n",
    "    ylim=y_range,\n",
    ")\n",
    "\n",
    "\n",
    "gv.tile_sources.CartoDark * violations"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
